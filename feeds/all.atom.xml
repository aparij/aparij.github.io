<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>My humble blog</title><link href="http://blog.alexparij.com/" rel="alternate"></link><link href="http://blog.alexparij.com/feeds/all.atom.xml" rel="self"></link><id>http://blog.alexparij.com/</id><updated>2014-04-11T10:20:00-04:00</updated><entry><title>Kaggle’s Asus Competition - got into top 25%</title><link href="http://blog.alexparij.com/kaggle-asus-failure-survival-analysis.html" rel="alternate"></link><updated>2014-04-11T10:20:00-04:00</updated><author><name>Alex Parij</name></author><id>tag:blog.alexparij.com,2014-04-11:kaggle-asus-failure-survival-analysis.html</id><summary type="html">&lt;p&gt;I recently finished participating in Kaggle&amp;#8217;s &lt;span class="caps"&gt;ASUS&lt;/span&gt; competition which was about predicting future malfunctional 
components of &lt;span class="caps"&gt;ASUS&lt;/span&gt; notebooks from historical data. My final placement in this competition was 140/614 that is the top 25%, for which I&amp;#8217;m very happy.
Considering the fact that it was more about time-series forecasting and less to do with core machine learning algorithms, I&amp;#8217;ve done&amp;nbsp;well. &lt;/p&gt;
&lt;p&gt;Basically I was given two data sources, one is sales and another repairs.
Sales data was from January/2005 to February/2008&amp;nbsp;:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;module_category&lt;/th&gt;
&lt;th&gt;component_category&lt;/th&gt;
&lt;th&gt;year/month&lt;/th&gt;
&lt;th&gt;number_sale&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;M4&lt;/td&gt;
&lt;td&gt;P10&lt;/td&gt;
&lt;td&gt;2007/1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;M4&lt;/td&gt;
&lt;td&gt;P27&lt;/td&gt;
&lt;td&gt;2005/5&lt;/td&gt;
&lt;td&gt;1042&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;M1&lt;/td&gt;
&lt;td&gt;P22&lt;/td&gt;
&lt;td&gt;2005/9&lt;/td&gt;
&lt;td&gt;1677&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Repairs data was from February/2005 to December/2009, for&amp;nbsp;example:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;module_category&lt;/th&gt;
&lt;th&gt;component_category&lt;/th&gt;
&lt;th&gt;year/month(sale)&lt;/th&gt;
&lt;th&gt;year/month(repair)&lt;/th&gt;
&lt;th&gt;number_repair&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;M6&lt;/td&gt;
&lt;td&gt;P16&lt;/td&gt;
&lt;td&gt;2007/9&lt;/td&gt;
&lt;td&gt;2009/4&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;M2&lt;/td&gt;
&lt;td&gt;P30&lt;/td&gt;
&lt;td&gt;2007/9&lt;/td&gt;
&lt;td&gt;2009/8&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;M1&lt;/td&gt;
&lt;td&gt;P12&lt;/td&gt;
&lt;td&gt;2006/10&lt;/td&gt;
&lt;td&gt;2008/2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;M1&lt;/td&gt;
&lt;td&gt;P30&lt;/td&gt;
&lt;td&gt;2006/5&lt;/td&gt;
&lt;td&gt;2007/7&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;M3&lt;/td&gt;
&lt;td&gt;P06&lt;/td&gt;
&lt;td&gt;2007/8&lt;/td&gt;
&lt;td&gt;2007/12&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;M7&lt;/td&gt;
&lt;td&gt;P19&lt;/td&gt;
&lt;td&gt;2006/7&lt;/td&gt;
&lt;td&gt;2007/6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;And I was trying to predict the monthly repair amount for each module-component from January/2010 to July/2011 (for 19 months) .
The prediction was evaluated using mean absolute error (&lt;span class="caps"&gt;MAE&lt;/span&gt;) which is the difference bertween the number of repairs I predicted 
vs real repairs data &lt;span class="caps"&gt;ASUS&lt;/span&gt; had, devided by total of prediction&amp;nbsp;rows.&lt;/p&gt;
&lt;p&gt;Just to explore the data using Python&amp;#8217;s Pandas DataFrame, grouping by module,component and&amp;nbsp;date,.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt; &lt;span class="n"&gt;repair_data&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;module_category&lt;/span&gt;&lt;span class="sc"&gt;&amp;#39;,&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;component_category&lt;/span&gt;&lt;span class="sc"&gt;&amp;#39;,&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;year&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;month&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;repair&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;as_index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;False&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;agg&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;number_repair&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Just one component&amp;#8217;s graph of the total repairs 2005 to 2009 time series will look like this&amp;nbsp;:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Repairs graph" src="images/repairs.png" /&gt;&lt;/p&gt;
&lt;p&gt;We can see that the number of repairs grows as more components are sold and also as the time passes, but falls off around two years mark 
as the first sold models reach that age. Not sure why the drop but my assumption was that the warranty expired.
 The number of repairs in the end of 2009 is zero or almost zero for most components and we need to predict what happens in the next 19 months.
The easiest would be just take the last points and fit some linear regression or moving average,
 that would bring us above the baseline(which is predict that we have only zero repairs) but it would not be the best&amp;nbsp;model. &lt;/p&gt;
&lt;p&gt;My most most successful model was built using a simple survival analysis
 (using Python&amp;#8217;s &lt;a href="https://github.com/CamDavidsonPilon/lifelines"&gt;Lifelines&lt;/a&gt; package) blended with linear regression for 
the tail of the 19 months to&amp;nbsp;forecast&lt;/p&gt;
&lt;p&gt;I took the time from sale of component to repair as time to death/event and the rest of the components were right censored(never had death event).
Didn&amp;#8217;t matter when the component was sold because it was all relative. I got let&amp;#8217;s say couple of thousands deaths with 1 to ~ 45 months from been
 sold to the repaired/death event and ~500k of right censored items and then estimated the hazard rates using &lt;a href="http://en.wikipedia.org/wiki/Nelson%E2%80%93Aalen_estimator"&gt;Nelson-Aalen estimator&lt;/a&gt; from
Lifelines library.
data would look like this, an array of deaths&amp;nbsp;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;  &lt;span class="n"&gt;data_events&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt; &lt;span class="mf"&gt;70.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
     &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
     &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
     &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
     &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
     &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
     &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;12.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;12.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;12.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;12.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;12.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;12.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="mf"&gt;12.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;12.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;12.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;12.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;12.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;12.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;12.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;12.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;12.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;12.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;12.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="mf"&gt;12.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;12.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;12.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;12.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;12.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;12.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;12.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;12.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;12.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;12.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;12.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="mf"&gt;12.&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;the first cell for example has a value of 70 it is a component that never was repaired but right censored, it can be any arbitrary high number. other cells with values like 1 or 12 or 3
is after how many months the component was repaired. If it was sold in September/2008 and repaired in September/2009 it means 12 months later death event occured. So I would  have arrays of size around 500k mostly with a value of 70 (component that never repaired).
Fitting the&amp;nbsp;data:&lt;/p&gt;
&lt;p&gt;naf = NelsonAalenFitter()
   naf.fit(data_events, event_observed=C&amp;nbsp;)&lt;/p&gt;
&lt;p&gt;C is the index of right censored components in the array (the ones with&amp;nbsp;70)&lt;/p&gt;
&lt;p&gt;What I get is the cumulative hazard rate, which is an integration of survival&amp;nbsp;function&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;  &lt;span class="n"&gt;naf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cumulative_hazard_&lt;/span&gt;

    &lt;span class="n"&gt;timeline&lt;/span&gt;   &lt;span class="n"&gt;NA&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;estimate&lt;/span&gt;          
    &lt;span class="mi"&gt;0&lt;/span&gt;            &lt;span class="mf"&gt;0.000000&lt;/span&gt;
    &lt;span class="mi"&gt;1&lt;/span&gt;            &lt;span class="mf"&gt;0.000071&lt;/span&gt;
    &lt;span class="mi"&gt;2&lt;/span&gt;            &lt;span class="mf"&gt;0.000339&lt;/span&gt;
    &lt;span class="mi"&gt;3&lt;/span&gt;            &lt;span class="mf"&gt;0.000785&lt;/span&gt;
    &lt;span class="mi"&gt;4&lt;/span&gt;            &lt;span class="mf"&gt;0.001369&lt;/span&gt;
    &lt;span class="mi"&gt;5&lt;/span&gt;            &lt;span class="mf"&gt;0.001962&lt;/span&gt;
    &lt;span class="p"&gt;...&lt;/span&gt;
    &lt;span class="p"&gt;...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="Hazard graph" src="images/hazard_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;What is important in this graph is the rate of change, in the beginning the slope is quite high that is basically gives high hazard rate at any moment(more repairs) and then after two years it would
stabilize and have almost a zero slope which is equivalent to no or very little repairs/death events.
let&amp;#8217;s say 10000 components were sold on Nov/2009 I need to know how many of them will be repaired in Feb/2010 which is 3 months after the sale. From the cumulative hazard  I can see that
at 3 months the cumulative hazard is 0.000785 and at 2 months  0.000339 , the slope will be 0.000785-0.000339=.000446 . Taking the population multiplied by instantenious hazard 
10000*0.000446 = 4.46 . On Feb/2010 4 components will be returned for repairs.
Based on the data, usually from 0 to 45 months it gave a nice prediction but for components that were sold earlier and I had to extrapolate how many will be repaired after 4-5 years I used a modified linear regression based only on the last couple of points and I would slowly decay the hazard value from this regression.  &lt;br /&gt;
I would also manually add extra weight for summer months, because it looked like in summer months the components failed&amp;nbsp;more.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Repairs graph" src="images/repairs_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;The red dots are the forecasted&amp;nbsp;repairs.&lt;/p&gt;
&lt;p&gt;The source for the Python implementation is on Github &lt;a href="https://github.com/aparij/kaggle_asus"&gt;Code&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;I also&amp;nbsp;tried:&lt;/p&gt;
&lt;p&gt;Aalen’s Additive model from survival analysis, which is a regression model with multiple covariabts. It was too slow and a bit worse results, 
maybe I chose bad covariants(sale seasons, months&amp;#8230;).
I read some participants managed to rank top spots using Cox model which is another regression model from survival&amp;nbsp;analysis.&lt;/p&gt;
&lt;p&gt;I also did &lt;span class="caps"&gt;VAR&lt;/span&gt; and &lt;span class="caps"&gt;ARMA&lt;/span&gt; from time series analysis in python&amp;#8217;s statsmodels but it worked badly or I just didn&amp;#8217;t understand it&amp;nbsp;properly&lt;/p&gt;
&lt;p&gt;Big thanks to Cameronon Davidson-Pilon for writing the excellent Lifelines package for&amp;nbsp;Python.&lt;/p&gt;</summary><category term="python"></category><category term="machinelearning"></category><category term="kaggle"></category></entry><entry><title>Kaggle’s Facebook competition</title><link href="http://blog.alexparij.com/kaggle-facebook-competition-keyword-extraction.html" rel="alternate"></link><updated>2013-12-21T10:20:00-05:00</updated><author><name>Alex Parij</name></author><id>tag:blog.alexparij.com,2013-12-21:kaggle-facebook-competition-keyword-extraction.html</id><summary type="html">&lt;p&gt;Kaggle&amp;#8217;s FacebookIII keyword extraction competition is over.
It was about predicting tags to StackExchange questions.
Check it out at &lt;a href="http://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction"&gt;Kaggle&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ve entered at a very late stage of the competition and didn&amp;#8217;t have time to properly experiment. 
My final standing was quite low 316/380, but I just had 3 weeks comparing to 4 months of the total competition&amp;#8217;s length, so I&amp;#8217;m &lt;span class="caps"&gt;OK&lt;/span&gt; with that.
It was also my first experience with Gensim python library (after reading &amp;#8220;Building Machine Learning Systems with Python&amp;#8221; by&amp;nbsp;Rciher,Coelho). &lt;/p&gt;
&lt;p&gt;I&amp;#8217;ve tried to model the question&amp;#8217;s topic using &lt;span class="caps"&gt;LDA&lt;/span&gt; &lt;a href="http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation"&gt;Wikipedia&lt;/a&gt;. For preprocessing I used &lt;span class="caps"&gt;NLTK&lt;/span&gt; with stemming, stop words and punctuation marks were removed. 
Removing less frequent words (e.g. encountered only once). Tried to remove code snippets.
Then I would find the most similar question based on the topics generated by &lt;span class="caps"&gt;LDA&lt;/span&gt; algo and take the tags from the training question to the test question and drop less frequent&amp;nbsp;tags&lt;/p&gt;
&lt;h1&gt;So what could have I done&amp;nbsp;better?&lt;/h1&gt;
&lt;p&gt;Preprocessing should be done only once, while I did every time I was building a dictionary or corpora. It would have probably sped 
it up by at least x2. 
More automatic parallezation(although I used &lt;span class="caps"&gt;MLK&lt;/span&gt; by Contunium Analytics to get some free optimization up to 4 threads without extra work on my part).
For example I could have made some simple code to split/load large test file or it  can be done manually using Linux split on test.csv and then &amp;#8220;cat * &amp;gt; out&amp;#8221; and combining the results.
&lt;span class="caps"&gt;POS&lt;/span&gt;(parts of speech) tagging was too slow(using &lt;span class="caps"&gt;NLTK&lt;/span&gt;) but I can see that with other speed improvements &lt;span class="caps"&gt;POS&lt;/span&gt; can be used to remove adjectives,adverbs and others, lowering the numbfer of features.
There were too many features (1mil words) for 3mil training entries! I should have tried to remove most of them. Removing words with freq=1 was not enough.
In the end for 3 mil training rows the similarity search was too slow and 2 mil test rows will take 48 hours to calculate.I moved to a high-end machines in an Amazon cloud. It did sped up things since it had 8 cores to work and I could just split the test file across processes.
For some reason &lt;span class="caps"&gt;MKL&lt;/span&gt; or &lt;span class="caps"&gt;BLAS&lt;/span&gt; run only with one thread, I tried to use StarCluster ec2 image that comes with all the SciPy libs preinstalled but still it would persistenly run on one&amp;nbsp;thread.&lt;/p&gt;
&lt;p&gt;Anyways , next time I should join the competition&amp;nbsp;earlier.&lt;/p&gt;
&lt;p&gt;The source is on &lt;a href="https://github.com/aparij/KaggleFacebookIII"&gt;Github&lt;/a&gt;&lt;/p&gt;</summary><category term="python"></category><category term="numpy"></category><category term="gensim"></category><category term="machinelearning"></category><category term="kaggle"></category></entry><entry><title>Numpy substring search indexed result</title><link href="http://blog.alexparij.com/numpy-array-string-search.html" rel="alternate"></link><updated>2013-05-29T10:20:00-04:00</updated><author><name>Alex Parij</name></author><id>tag:blog.alexparij.com,2013-05-29:numpy-array-string-search.html</id><summary type="html">&lt;p&gt;Working on Kaggle’s Titanic competition I needed to test each Numpy array cell if the string s1 contains the second string s2 and return an indexed array with True/False values.
Let&amp;#8217;s define an&amp;nbsp;array:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt; &lt;span class="n"&gt;as&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;nparr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;aaMRac&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;bbbb&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;ccc&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;ffff&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;eeee&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;gggggg&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;nparr&lt;/span&gt;
&lt;span class="n"&gt;Out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; 
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;aaMRac&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;bbbb&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;ccc&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;ffff&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;eeee&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;gggggg&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt; 
      &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;|S6&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and I’m looking for strings that contain ‘&lt;span class="caps"&gt;MR&lt;/span&gt;’. I should get&amp;nbsp;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;False&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;False&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;False&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;because ‘aaMRac’ is the only cell that one contains ‘&lt;span class="caps"&gt;MR&lt;/span&gt;’.
Trying&amp;nbsp;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;MR&amp;quot;&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;nparr&lt;/span&gt;
&lt;span class="n"&gt;Out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;False&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Gives me False because it tests for a string to string equality and returns the answer for the entire&amp;nbsp;array.&lt;/p&gt;
&lt;p&gt;To get the indexed answer I&amp;nbsp;do&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;MR&amp;quot;&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;nparr&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flat&lt;/span&gt;&lt;span class="p"&gt;]).&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nparr&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; 
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;False&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
   &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;False&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;False&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kt"&gt;bool&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;which flattens the array before looking for the substring using a list comprehension. It then creates the new indexed answer with the right array&amp;nbsp;dimensions.&lt;/p&gt;
&lt;p&gt;If you want to select only one column, you do like&amp;nbsp;this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;MR&amp;quot;&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;nparr&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="n"&gt;flat&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;Out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;False&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kt"&gt;bool&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</summary><category term="python"></category><category term="numpy"></category></entry></feed>